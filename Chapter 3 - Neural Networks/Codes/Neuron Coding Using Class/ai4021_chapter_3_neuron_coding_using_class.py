# -*- coding: utf-8 -*-
"""AI4021 - Chapter 3 - Neuron Coding Using Class.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HVlh4YI8i_25RswtEH88rRNoMe5CzrmL

# **Imports**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.datasets import make_classification, make_regression
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

from mlxtend.plotting import plot_decision_regions

import __main__

"""# **Load Dataset**"""

X, y = make_classification(n_samples=1000, n_features=2,
                           n_redundant=0, n_clusters_per_class=1, class_sep=2.0,
                           n_classes=2, random_state=27)
print(X.shape, y.shape)

plt.scatter(X[:, 0], X[:, 1], c=y)

"""### Train-Test Split"""

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

x_train.shape, x_test.shape, y_train.shape, y_test.shape

"""# **OOP Style**"""

model = LogisticRegression(penalty='none')
model.fit(x_train, y_train)
model.predict(x_test)
model.score(x_test, y_test)

model.intercept_, model.coef_

"""# OOP Style"""

class ModelName:

    def __init__(self, penalty, ...):
        pass

    def fit(self, x, y):
        pass

    def predict(self, x):
        pass

    def score(self, x, y):
        pass

"""# **Neuron (from Scratch)**

## Activation Function
"""

def relu(x):
    return np.maximum(0, x)

def sigmoid(x):
    return 1/(1+np.exp(-x))

def tanh(x):
    pass

"""## Loss"""

def bce(y, y_hat):
    return np.mean(-(y*np.log(y_hat) + (1-y)*np.log(1-y_hat)))

def mse(y, y_hat):
    return np.mean((y - y_hat)**2)

"""## Accuracy"""

def accuracy(y, y_hat, t=0.5):
    y_hat = np.where(y_hat<t, 0, 1)
    acc = np.sum(y == y_hat) / len(y)
    return acc

"""## Neuron"""

class Neuron:

    def __init__(self, in_features, af=None, loss_fn=mse, n_iter=100, eta=0.1, verbose=True):
        self.in_features = in_features
        # weight & bias
        self.w = np.random.randn(in_features, 1)
        self.b = np.random.randn()
        self.af = af
        self.loss_fn = loss_fn
        self.loss_hist = []
        self.w_grad, self.b_grad = None, None
        self.n_iter = n_iter
        self.eta = eta
        self.verbose = verbose

    def predict(self, x):
        # x: [n_samples, in_features]
        y_hat = x @ self.w + self.b
        y_hat = y_hat if self.af is None else self.af(y_hat)
        return y_hat

    def fit(self, x, y):
        for i in range(self.n_iter):
            y_hat = self.predict(x)
            loss = self.loss_fn(y, y_hat)
            self.loss_hist.append(loss)
            self.gradient(x, y, y_hat)
            self.gradient_descent()
            if self.verbose & (i % 10 == 0):
                print(f'Iter={i}, Loss={loss:.4}')

    def gradient(self, x, y, y_hat):
        self.w_grad = (x.T @ (y_hat - y)) / len(y)
        self.b_grad = (y_hat - y).mean()

    def gradient_descent(self):
        self.w -= self.eta * self.w_grad
        self.b -= self.eta * self.b_grad

    def __repr__(self):
        return f'Neuron({self.in_features}, {self.af.__name__})'

    def parameters(self):
        return {'w': self.w, 'b': self.b}

neuron = Neuron(in_features=2, af=sigmoid)
neuron.predict(X)
print(neuron)
print(neuron.parameters())

neuron.fit(x_train, y_train[:, None])
neuron.parameters()

plt.plot(neuron.loss_hist)

"""## Train"""

neuron = Neuron(2, af=sigmoid, loss_fn=bce, n_iter=500)
neuron.fit(X, y[:, None])

plt.plot(neuron.loss_hist)

"""## Evaluation"""

y_hat = neuron.predict(x_test)
accuracy(y_test[:, None], y_hat, t=0.5)

y_hat[:, 0], y_test

"""## Plot"""

plot_decision_regions(x_train, y_train, clf=neuron)