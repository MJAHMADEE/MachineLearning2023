# -*- coding: utf-8 -*-
"""AI4021 - Chapter 6_2 - SVM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bqPojlcG9FRLIhkJE2lEiEx7pZoLoqyr

# <font color='#FFE15D'>**SVM âšž**</font>

## **ðŸ”¸ Imports**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.datasets import make_classification, make_regression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

"""## **ðŸ”¸ Perceptron (from Scratch)**

### Data
"""

X, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0,
                           n_clusters_per_class=1, class_sep=2., random_state=12)
y = np.where(y==0, -1, y)[:, None]

plt.scatter(X[:, 0], X[:, 1], c=y, cmap='jet')

y

"""### Model"""

class Perceptron:

    def __init__(self, in_features, n_iter=1000, eta=0.01, random_state=42):
        np.random.seed(random_state)
        self.w = np.random.randn(in_features, 1)
        self.b = np.random.randn()
        self.n_iter = n_iter
        self.eta = eta
        self.loss_hist = []

    def fit(self, X, y):
        # train loop
        for i in range(self.n_iter):
            y_hat = self.predict(X)
            loss = self._loss(y, y_hat)
            grad_w, grad_b = self._grad(X, y, y_hat)
            self.w -= self.eta * grad_w
            self.b -= self.eta * grad_b
            self.loss_hist.append(loss)

    def predict(self, x):
        return x @ self.w + self.b

    def score(self, X, y):
        y_hat = self.predict(X)
        return self._accuracy(y, y_hat, t=0)

    def _accuracy(self, y, y_hat, t=0):
        y_hat = np.where(y_hat<t, -1, 1)
        acc = np.sum(y == y_hat) / len(y)
        return acc

    def _loss(self, y, y_hat):
        return np.maximum(0, -y*y_hat).mean()

    def _grad(self, x, y, y_hat):
        grad_w = (-y*x * np.heaviside(-y*y_hat, 1)).mean(axis=0).reshape(self.w.shape)
        grad_b = (-y * np.heaviside(-y*y_hat, 1)).mean(axis=0)
        return grad_w, grad_b

"""### Train"""

model = Perceptron(in_features=2, n_iter=1000, eta=0.01, random_state=2)
model.fit(X, y)

plt.plot(model.loss_hist)

model.loss_hist[-1]

"""### Evaluation"""

model.score(X, y)

"""### Decision Boundary"""

n = 1000
xmin, xmax = X.min(0), X.max(0)
x1r = np.linspace(xmin[0], xmax[0], n)
x2r = np.linspace(xmin[1], xmax[1], n)
x1m, x2m = np.meshgrid(x1r, x2r)

Xm = np.stack((x1m.flatten(), x2m.flatten()), axis=1)
ym = model.predict(Xm)
ym = ym.reshape(x1m.shape)

plt.contour(x1m, x2m, ym, levels=[0], linewidths=3, colors='green')

plt.scatter(X[:, 0], X[:, 1], c=y, cmap='jet', s=50, zorder=3)

"""## **ðŸ”¸ Simplified SVM (from Scratch)**

### Data
"""

X, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0,
                           n_clusters_per_class=1, class_sep=3., random_state=12)

flags = np.ones((1000), dtype=np.bool_)
flags[[154, 770]] = False
X = X[flags, :]
y = y[flags]
print(X.shape, y.shape)
y = np.where(y==0, -1, y)[:, None]

plt.scatter(X[:, 0], X[:, 1], c=y, cmap='jet');

"""### Model"""

class SVM:

    def __init__(self, in_features, n_iter=1000, eta=0.01, c=1., random_state=42):
        # np.random.seed(random_state)
        self.w = np.random.randn(in_features, 1)
        self.b = np.random.randn()
        # self.w = np.zeros((in_features, 1))
        # self.b = 0
        self.n_iter = n_iter
        self.eta = eta
        self.c = c
        self.loss_hist = []

    def fit(self, X, y):
        # train loop
        for i in range(self.n_iter):
            y_hat = self.predict(X)
            # false predictions
            mask = np.squeeze((1 - y*y_hat) > 0)
            if mask.sum() == 0:
                print(i, 'break!')
                break
            loss = self._loss(y, y_hat, mask)
            grad_w, grad_b = self._grad(X, y, y_hat, mask)
            self.w -= self.eta * grad_w
            self.b -= self.eta * grad_b
            self.loss_hist.append(loss)

    def predict(self, x):
        return x @ self.w + self.b

    def score(self, X, y):
        y_hat = self.predict(X)
        return self._accuracy(y, y_hat, t=0)

    def _accuracy(self, y, y_hat, t=0):
        y_hat = np.where(y_hat<t, -1, 1)
        acc = np.sum(y == y_hat) / len(y)
        return acc

    def _loss(self, y, y_hat, mask):
        y_mask = y[mask]
        y_hat_mask = y_hat[mask]
        return np.maximum(0, 1-y_mask*y_hat_mask).mean()

    def _grad(self, x, y, y_hat, mask):
        x_mask = x[mask]
        y_mask = y[mask]
        # y_hat_mask = y_hat[mask]
        grad_w = (-y_mask*x_mask).mean(axis=0).reshape(self.w.shape) + self.c*self.w
        grad_b = (-y_mask).mean(axis=0)
        return grad_w, grad_b

"""### Train"""

model = SVM(in_features=2, n_iter=2000, eta=0.01, c=1., random_state=2)
model.fit(X, y)

plt.plot(model.loss_hist)

model.loss_hist[-1]

"""### Evaluation"""

model.score(X, y)

"""### Decision Boundary"""

n = 1000
xmin, xmax = X.min(0), X.max(0)
x1r = np.linspace(xmin[0], xmax[0], n)
x2r = np.linspace(xmin[1], xmax[1], n)
x1m, x2m = np.meshgrid(x1r, x2r)

Xm = np.stack((x1m.flatten(), x2m.flatten()), axis=1)
ym = model.predict(Xm)
ym = ym.reshape(x1m.shape)

# plt.contourf(x1m, x2m, ym, levels=[-20, -1, 1, 20], cmap='plasma')
plt.contour(x1m, x2m, ym, levels=[-1, 0, 1], linestyles=['--', '-', '--'], linewidths=3, colors='green')

plt.scatter(X[:, 0], X[:, 1], c=y, cmap='jet', s=50, zorder=3)

"""### `C`"""

model = SVM(in_features=2, n_iter=2000, eta=0.01, c=5.5, random_state=2)
model.fit(X, y)

n = 1000
xmin, xmax = X.min(0), X.max(0)
x1r = np.linspace(xmin[0], xmax[0], n)
x2r = np.linspace(xmin[1], xmax[1], n)
x1m, x2m = np.meshgrid(x1r, x2r)

Xm = np.stack((x1m.flatten(), x2m.flatten()), axis=1)
ym = model.predict(Xm)
ym = ym.reshape(x1m.shape)

plt.contour(x1m, x2m, ym, levels=[-1, 0, 1], linestyles=['--', '-', '--'], linewidths=3, colors='green')

plt.scatter(X[:, 0], X[:, 1], c=y, cmap='jet', s=50, zorder=3)

"""## **ðŸ”¸ Simplified SVM (sklearn)**"""

from sklearn.linear_model import SGDClassifier

X, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0,
                           n_clusters_per_class=1, class_sep=3., random_state=12)

# flags = np.ones((1000), dtype=np.bool_)
# flags[[154, 770]] = False
# X = X[flags, :]
# y = y[flags]

plt.scatter(X[:, 0], X[:, 1], c=y, cmap='jet');

model = SGDClassifier(loss='hinge', penalty='l2', alpha=0.001, tol=0.0001, random_state=23)
model.fit(X, y)

n = 1000
xmin, xmax = X.min(0), X.max(0)
x1r = np.linspace(xmin[0], xmax[0], n)
x2r = np.linspace(xmin[1], xmax[1], n)
x1m, x2m = np.meshgrid(x1r, x2r)

Xm = np.stack((x1m.flatten(), x2m.flatten()), axis=1)
ym = model.decision_function(Xm)
ym = ym.reshape(x1m.shape)

# plt.contourf(x1m, x2m, ym, levels=[-20, -1, 1, 20], cmap='plasma')
plt.contour(x1m, x2m, ym, levels=[-1, 0, 1], linestyles=['--', '-', '--'], linewidths=3, colors='green')

plt.scatter(X[:, 0], X[:, 1], c=y, cmap='jet', s=50, zorder=3)

model = SGDClassifier(loss='hinge', penalty='l2', alpha=1, max_iter=2000, tol=0.0001, random_state=23)
model.fit(X, y)

n = 1000
xmin, xmax = X.min(0), X.max(0)
x1r = np.linspace(xmin[0], xmax[0], n)
x2r = np.linspace(xmin[1], xmax[1], n)
x1m, x2m = np.meshgrid(x1r, x2r)

Xm = np.stack((x1m.flatten(), x2m.flatten()), axis=1)
ym = model.decision_function(Xm)
ym = ym.reshape(x1m.shape)

# plt.contourf(x1m, x2m, ym, levels=[-20, -1, 1, 20], cmap='plasma')
plt.contour(x1m, x2m, ym, levels=[-1, 0, 1], linestyles=['--', '-', '--'], linewidths=3, colors='green')

plt.scatter(X[:, 0], X[:, 1], c=y, cmap='jet', s=50, zorder=3)

"""## **ðŸ”¸ SVM (sklearn)**"""

from sklearn.svm import LinearSVC, SVC, NuSVC

X, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0,
                           n_clusters_per_class=1, class_sep=3., random_state=12)

# flags = np.ones((1000), dtype=np.bool_)
# flags[[154, 770]] = False
# X = X[flags, :]
# y = y[flags]

plt.scatter(X[:, 0], X[:, 1], c=y, cmap='jet');

"""### `LinearSVC`"""

model = LinearSVC(loss='hinge', C=0.1, max_iter=2000)
model.fit(X, y)

n = 1000
xmin, xmax = X.min(0), X.max(0)
x1r = np.linspace(xmin[0], xmax[0], n)
x2r = np.linspace(xmin[1], xmax[1], n)
x1m, x2m = np.meshgrid(x1r, x2r)

Xm = np.stack((x1m.flatten(), x2m.flatten()), axis=1)
ym = model.decision_function(Xm)
ym = ym.reshape(x1m.shape)

# plt.contourf(x1m, x2m, ym, levels=[-20, -1, 1, 20], cmap='plasma')
plt.contour(x1m, x2m, ym, levels=[-1, 0, 1], linestyles=['--', '-', '--'], linewidths=3, colors='green')

plt.scatter(X[:, 0], X[:, 1], c=y, cmap='jet', s=50, zorder=3)

model = LinearSVC(loss='hinge', C=1.2, max_iter=2000)
model.fit(X, y)

n = 1000
xmin, xmax = X.min(0), X.max(0)
x1r = np.linspace(xmin[0], xmax[0], n)
x2r = np.linspace(xmin[1], xmax[1], n)
x1m, x2m = np.meshgrid(x1r, x2r)

Xm = np.stack((x1m.flatten(), x2m.flatten()), axis=1)
ym = model.decision_function(Xm)
ym = ym.reshape(x1m.shape)

# plt.contourf(x1m, x2m, ym, levels=[-20, -1, 1, 20], cmap='plasma')
plt.contour(x1m, x2m, ym, levels=[-1, 0, 1], linestyles=['--', '-', '--'], linewidths=3, colors='green')

plt.scatter(X[:, 0], X[:, 1], c=y, cmap='jet', s=50, zorder=3)

model.coef_, model.intercept_

"""### `SVC`"""

model = SVC(C=1., kernel='linear', probability=True)
model.fit(X, y)

n = 1000
xmin, xmax = X.min(0), X.max(0)
x1r = np.linspace(xmin[0], xmax[0], n)
x2r = np.linspace(xmin[1], xmax[1], n)
x1m, x2m = np.meshgrid(x1r, x2r)

Xm = np.stack((x1m.flatten(), x2m.flatten()), axis=1)
ym = model.decision_function(Xm)
ym = ym.reshape(x1m.shape)

# plt.contourf(x1m, x2m, ym, levels=[-20, -1, 1, 20], cmap='plasma')
plt.contour(x1m, x2m, ym, levels=[-1, 0, 1], linestyles=['--', '-', '--'], linewidths=3, colors='green')

plt.scatter(X[:, 0], X[:, 1], c=y, cmap='jet', s=50, zorder=3)

sv = model.support_vectors_
plt.scatter(sv[:, 0], sv[:, 1], zorder=4, c='none', s=100, edgecolors='y')

model.coef_, model.intercept_

model.support_, model.n_support_, model.support_vectors_

model.dual_coef_

"""### `NuSVC`"""

model = NuSVC(nu=0.95, kernel='linear', probability=True)
model.fit(X, y)

n = 1000
xmin, xmax = X.min(0), X.max(0)
x1r = np.linspace(xmin[0], xmax[0], n)
x2r = np.linspace(xmin[1], xmax[1], n)
x1m, x2m = np.meshgrid(x1r, x2r)

Xm = np.stack((x1m.flatten(), x2m.flatten()), axis=1)
ym = model.decision_function(Xm)
ym = ym.reshape(x1m.shape)

# plt.contourf(x1m, x2m, ym, levels=[-20, -1, 1, 20], cmap='plasma')
plt.contour(x1m, x2m, ym, levels=[-1, 0, 1], linestyles=['--', '-', '--'], linewidths=3, colors='green')

plt.scatter(X[:, 0], X[:, 1], c=y, cmap='jet', s=50, zorder=3)

sv = model.support_vectors_
plt.scatter(sv[:, 0], sv[:, 1], zorder=4, c='none', s=100, edgecolors='y')

model.predict_proba(X)

"""## **ðŸ”¸ Kernel Trick (sklearn)**"""

from sklearn.svm import SVC, NuSVC

data = np.loadtxt('/content/ex2data1.txt', delimiter=',')

X = data[:, :-1].copy()
y = data[:, -1].copy().astype(np.int64)

normz = StandardScaler()
X = normz.fit_transform(X)

plt.scatter(X[:, 0], X[:, 1], c=y, cmap='jet');

"""### `SVC`"""

# model = SVC(C=0.1, kernel='rbf', gamma = 'auto')
# model.fit(X, y)

model = SVC(C=1., kernel='poly', degree=2, coef0=1)
model.fit(X, y)

n = 1000
xmin, xmax = X.min(0), X.max(0)
x1r = np.linspace(xmin[0], xmax[0], n)
x2r = np.linspace(xmin[1], xmax[1], n)
x1m, x2m = np.meshgrid(x1r, x2r)

Xm = np.stack((x1m.flatten(), x2m.flatten()), axis=1)
ym = model.decision_function(Xm)
ym = ym.reshape(x1m.shape)

plt.contour(x1m, x2m, ym, levels=[0], linewidths=3, colors='green')

plt.scatter(X[:, 0], X[:, 1], c=y, cmap='jet', s=50, zorder=3)

sv = model.support_vectors_
plt.scatter(sv[:, 0], sv[:, 1], zorder=4, c='none', s=100, edgecolors='y')

"""#### Exercise"""

data = np.loadtxt('/content/ex2data2.txt', delimiter=',')

X = data[:, :-1].copy()
y = data[:, -1].copy().astype(np.int64)

normz = StandardScaler()
X = normz.fit_transform(X)

plt.scatter(X[:, 0], X[:, 1], c=y, cmap='jet');

"""### `NuSVC`"""

