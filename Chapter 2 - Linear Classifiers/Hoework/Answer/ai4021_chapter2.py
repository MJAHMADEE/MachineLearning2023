# -*- coding: utf-8 -*-
"""AI4021 - Chapter2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jUE2-FRYmot8ok-QxdPwsUYaO4U6Ab3z

##  Imports and Upload Data
"""

# from google.colab import drive
# drive.mount('/content/drive')

import os

# Define the folder name
folder_name = "Data"

# Check if the folder already exists, and create it if not
if not os.path.exists(folder_name):
    os.makedirs(folder_name)

# Commented out IPython magic to ensure Python compatibility.
# %cd Data

!pip install --upgrade --no-cache-dir gdown
!gdown 18ETECLmR2FqvJEFmVXJO4Kz9H8efFpRC
!gdown 1as4n8Q8fPZd_TxElKZzKdh336y1mpMP8
!gdown 1OWZpC_09GTfyGNUlq5mWA_6yRdPJQHiQ

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

"""#  Logistic Regression (from Scratch)

## 1D

### Load Dataset - 1D
"""

df = pd.read_csv('/content/Game.csv')
df

import pandas as pd
import numpy as np

# Read the CSV file into a DataFrame
df = pd.read_csv('/content/Game3.csv')

# Show the number of null values in each column
null_counts = df.isnull().sum()
print("Number of null values in each column:")
print(null_counts)

# Remove rows with null values
df = df.dropna()

# Reset the index after removing rows
df = df.reset_index(drop=True)

# Now, the DataFrame 'df' contains no rows with null values
print("DataFrame after removing rows with null values:")
print(df)

# import pandas as pd

# # Read the CSV file into a DataFrame
# df = pd.read_csv('/content/Game.csv')

# # Show the number of null values in each column
# null_counts = df.isnull().sum()
# print("Number of null values in each column:")
# print(null_counts)

# # Remove rows with null values
# df_clean = df.dropna()

# # Display the DataFrame after removing null values
# print("DataFrame after removing null values:")
# print(df_clean)

# df.info()
# df.isnull().sum()
# for i in df.columns:
#   print('Number of NaN in',i,'=',df.isna().sum().sum())

import seaborn as sns

# Exclude the last column from the DataFrame
df = df.iloc[:, :-1]

# Calculate correlation matrix
corr_matrix = df.corr()

# Create heatmap using seaborn
plt.figure(figsize=(25,25))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', linewidths=0.5, annot_kws={"size": 8}, fmt='.3f', yticklabels=corr_matrix.columns)

# Adjust font size of annotations
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)

# Adjust margins of PDF file
plt.savefig('PIcS1.pdf', bbox_inches='tight')

import seaborn as sns


# Calculate correlation matrix
corr_matrix = df.corr()

# Create heatmap using seaborn with a colormap from -1 to 1
plt.figure(figsize=(10, 10))  # Adjust the figure size as needed
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', linewidths=0.5, annot_kws={"size": 8}, fmt='.3f',
            yticklabels=corr_matrix.columns, vmin=-1, vmax=1)  # Set vmin and vmax

# Adjust font size of annotations
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)

# Adjust margins of PDF file
plt.savefig('PIcS1.pdf', bbox_inches='tight')

"""###  Train-Test Split"""

from sklearn.model_selection import train_test_split

X = df[['p_hours']].values
y = df[['w_l']].values
X, y

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

x_train.shape, x_test.shape, y_train.shape, y_test.shape

"""###  Logistic Regression (from Scratch)

#### Logistic Regression Model

$\hat{y}=\sigma(x)=\frac{1}{1+e^{-Xw}}$
"""

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def logistic_regression(x, w):
    y_hat = sigmoid(x @ w)
    return y_hat

y_hat = logistic_regression(np.random.randn(5, 2), np.random.randn(2, 1))

y_hat

"""#### Binary Cross Entropy (BCE)

$L=-[ylog(\hat{y})+(1-y)log(1-\hat{y})]$
"""

def bce(y, y_hat):
    loss = -(np.mean(y*np.log(y_hat) + (1-y)*np.log(1-y_hat)))
    return loss

bce(np.ones((5, 1)), y_hat)

"""#### Gradient

$\nabla L_w(w)=\frac{1}{n}X^T(\hat{y}-y)$
"""

def gradient(x, y, y_hat):
    grads = (x.T @ (y_hat - y)) / len(y)
    return grads

gradient(np.random.randn(5, 2), np.ones((5, 1)), y_hat)

"""#### Gradient Descent"""

def gradient_descent(w, eta, grads):
    w -= eta*grads
    return w

"""#### Accuracy"""

def accuracy(y, y_hat):
    acc = np.sum(y == np.round(y_hat)) / len(y)
    return acc

accuracy(np.array([1, 0, 1]), np.array([0.7, 0.4, 0.3]))

"""#### Train"""

x_train = np.hstack((np.ones((len(x_train), 1)), x_train))
x_train.shape

m = 1
w = np.random.randn(m+1, 1)
print(w.shape)

eta = 0.01
n_epochs = 2000 #N

error_hist = []

for epoch in range(n_epochs):
    # predictions
    y_hat = logistic_regression(x_train, w)

    # loss
    e = bce(y_train, y_hat)
    error_hist.append(e)

    # gradients
    grads = gradient(x_train, y_train, y_hat)

    # gradient descent
    w = gradient_descent(w, eta, grads)

    if (epoch+1) % 1 == 0:
        print(f'Epoch={epoch}, \t E={e:.4},\t w={w.T[0]}')

plt.plot(error_hist)

"""#### Test"""

x_test = np.hstack((np.ones((len(x_test), 1)), x_test))
x_test.shape

y_hat = logistic_regression(x_test, w)
accuracy(y_test, y_hat)

y_hat, y_test

"""## 1D - Normal

###  Load Dataset - 1D - Normal
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Read the CSV file into a DataFrame
df = pd.read_csv('/content/Game.csv')

# Split the data into training and test sets
X = df[['p_hours']].values
y = df[['w_l']].values
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a StandardScaler instance
scaler = StandardScaler()

# Fit the scaler on the training data and transform it
x_train_normalized = scaler.fit_transform(x_train)

# Transform the test data using the same scaler
x_test_normalized = scaler.transform(x_test)

# Check the shapes of the normalized data
x_train_normalized.shape, x_test_normalized.shape, y_train.shape, y_test.shape

"""###  Logistic Regression (from Scratch)

#### Logistic Regression Model

$\hat{y}=\sigma(x)=\frac{1}{1+e^{-Xw}}$
"""

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def logistic_regression(x, w):
    y_hat = sigmoid(x @ w)
    return y_hat

y_hat = logistic_regression(np.random.randn(5, 2), np.random.randn(2, 1))

y_hat

"""#### Binary Cross Entropy (BCE)

$L=-[ylog(\hat{y})+(1-y)log(1-\hat{y})]$
"""

def bce(y, y_hat):
    loss = -(np.mean(y*np.log(y_hat) + (1-y)*np.log(1-y_hat)))
    return loss

bce(np.ones((5, 1)), y_hat)

"""#### Gradient

$\nabla L_w(w)=\frac{1}{n}X^T(\hat{y}-y)$
"""

def gradient(x, y, y_hat):
    grads = (x.T @ (y_hat - y)) / len(y)
    return grads

gradient(np.random.randn(5, 2), np.ones((5, 1)), y_hat)

"""#### Gradient Descent"""

def gradient_descent(w, eta, grads):
    w -= eta*grads
    return w

"""#### Accuracy"""

def accuracy(y, y_hat):
    acc = np.sum(y == np.round(y_hat)) / len(y)
    return acc

accuracy(np.array([1, 0, 1]), np.array([0.7, 0.5, 0.3]))

"""#### Train"""

x_train = np.hstack((np.ones((len(x_train), 1)), x_train))
x_train.shape

m = 1
w = np.random.randn(m+1, 1)
print(w.shape)

eta = 0.01
n_epochs = 2000

error_hist = []

for epoch in range(n_epochs):
    # predictions
    y_hat = logistic_regression(x_train, w)

    # loss
    e = bce(y_train, y_hat)
    error_hist.append(e)

    # gradients
    grads = gradient(x_train, y_train, y_hat)

    # gradient descent
    w = gradient_descent(w, eta, grads)

    if (epoch+1) % 1 == 0:
        print(f'Epoch={epoch}, \t E={e:.4},\t w={w.T[0]}')

plt.plot(error_hist)

"""#### Test"""

x_test = np.hstack((np.ones((len(x_test), 1)), x_test))
x_test.shape

y_hat = logistic_regression(x_test, w)
accuracy(y_test, y_hat)

y_hat, y_test

"""## 2D

###  Load Dataset - 2D
"""

df = pd.read_csv('/content/Game2.csv')
df

"""###  Train-Test Split"""

from sklearn.model_selection import train_test_split

X = df[['p_hours', 'Esteda']].values
y = df[['w_l']].values
X, y

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

x_train.shape, x_test.shape, y_train.shape, y_test.shape

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Read the CSV file into a DataFrame
df = pd.read_csv('/content/Game2.csv')

# Split the data into training and test sets
# X = df[['p_hours', 'Esteda']].values
# y = df[['w_l']].values


# Remove columns 1, 4, and 5 (0-based index)
columns_to_remove = [0]
df = df.drop(df.columns[columns_to_remove], axis=1)

# Remove rows 1 to 3 and 7 to 9 (0-based index)
rows_to_remove = list(range(1, 4)) + list(range(7, 10))
df = df.drop(df.index[rows_to_remove])

# Split the data into features (X) and the target variable (y)
X = df.iloc[:, :-1].values  # Assuming the last column is the target variable
y = df.iloc[:, -1].values


x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a StandardScaler instance
scaler = StandardScaler()

# Fit the scaler on the training data and transform it
x_train_normalized = scaler.fit_transform(x_train)

# Transform the test data using the same scaler
x_test_normalized = scaler.transform(x_test)

# Check the shapes of the normalized data
x_train_normalized.shape, x_test_normalized.shape, y_train.shape, y_test.shape

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import numpy as np

# Read the CSV file into a DataFrame
df = pd.read_csv('/content/Game2.csv')

# Split the data into training and test sets
X = df[['p_hours', 'Esteda']].values
y = df[['w_l']].values
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create separate StandardScaler instances for each feature
scaler_p_hours = StandardScaler()
scaler_Esteda = StandardScaler()

# Fit and transform each feature separately
x_train_normalized_p_hours = scaler_p_hours.fit_transform(x_train[:, [0]])
x_train_normalized_Esteda = scaler_Esteda.fit_transform(x_train[:, [1]])

# Transform the corresponding test data using the same scalers
x_test_normalized_p_hours = scaler_p_hours.transform(x_test[:, [0]])
x_test_normalized_Esteda = scaler_Esteda.transform(x_test[:, [1]])

# Concatenate the normalized features back into a 2D array
x_train_normalized = np.hstack((x_train_normalized_p_hours, x_train_normalized_Esteda))
x_test_normalized = np.hstack((x_test_normalized_p_hours, x_test_normalized_Esteda))

# Check the shapes of the normalized data
x_train_normalized.shape, x_test_normalized.shape, y_train.shape, y_test.shape

"""###  Logistic Regression (from Scratch)

#### Logistic Regression Model

$\hat{y}=\sigma(x)=\frac{1}{1+e^{-Xw}}$
"""

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def logistic_regression(x, w):
    y_hat = sigmoid(x @ w)
    return y_hat

y_hat = logistic_regression(np.random.randn(5, 2), np.random.randn(2, 1))

y_hat

"""#### Binary Cross Entropy (BCE)

$L=-[ylog(\hat{y})+(1-y)log(1-\hat{y})]$
"""

def bce(y, y_hat):
    loss = -(np.mean(y*np.log(y_hat) + (1-y)*np.log(1-y_hat)))
    return loss

bce(np.ones((5, 1)), y_hat)

"""#### Gradient

$\nabla L_w(w)=\frac{1}{n}X^T(\hat{y}-y)$
"""

def gradient(x, y, y_hat):
    grads = (x.T @ (y_hat - y)) / len(y)
    return grads

gradient(np.random.randn(5, 2), np.ones((5, 1)), y_hat)

"""#### Gradient Descent"""

def gradient_descent(w, eta, grads):
    w -= eta*grads
    return w

"""#### Accuracy"""

def accuracy(y, y_hat):
    acc = np.sum(y == np.round(y_hat)) / len(y)
    return acc

accuracy(np.array([1, 0, 1]), np.array([0.7, 0.5, 0.3]))

"""#### Train"""

x_train = np.hstack((np.ones((len(x_train), 1)), x_train))
x_train.shape

m = 2
w = np.random.randn(m+1, 1)
print(w.shape)

eta = 0.01
n_epochs = 2000

error_hist = []

for epoch in range(n_epochs):
    # predictions
    y_hat = logistic_regression(x_train, w)

    # loss
    e = bce(y_train, y_hat)
    error_hist.append(e)

    # gradients
    grads = gradient(x_train, y_train, y_hat)

    # gradient descent
    w = gradient_descent(w, eta, grads)

    if (epoch+1) % 1 == 0:
        print(f'Epoch={epoch}, \t E={e:.4},\t w={w.T[0]}')

plt.plot(error_hist)

"""#### Test"""

x_test = np.hstack((np.ones((len(x_test), 1)), x_test))
x_test.shape

y_hat = logistic_regression(x_test, w)
accuracy(y_test, y_hat)

y_hat, y_test

"""#  Generate Dataset"""

from sklearn.datasets import load_breast_cancer, make_classification, make_blobs, make_circles, load_digits

"""## Breast Cancer Dataset"""

load_breast_cancer()

X, y = load_breast_cancer(return_X_y=True)
X.shape, y.shape

"""## Synthetic dataset"""

import matplotlib.pyplot as plt

X, y = make_classification(n_samples=1000, n_features=2, n_redundant=0, n_clusters_per_class=1, class_sep=0.1, n_classes=2, random_state=27)
print(X.shape, y.shape)

plt.scatter(X[:, 0], X[:, 1], c=y)

make_circles()

X, y = make_circles(n_samples=1000, noise=0.01)

plt.scatter(X[:, 0], X[:, 1], c=y)

"""# **Logistic Regression (sklearn)**"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression, SGDClassifier

X, y = load_breast_cancer(return_X_y=True)
X.shape, y.shape

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
x_train.shape, y_train.shape, x_test.shape, y_test.shape

"""### `LogisticRegression()`"""

model = LogisticRegression(solver='sag', max_iter=200, random_state=14)
model.fit(x_train, y_train)
model.predict(x_test), y_test

model.predict_proba(x_test)

model.predict_log_proba(x_test)

model.score(x_train, y_train)

model.score(x_test, y_test)

"""### `SGDClassifier()`"""

model = SGDClassifier(loss='log_loss', random_state=27)
model.fit(x_train, y_train)

model.score(x_train, y_train)

model.score(x_test, y_test)

"""#  Decision Boundary

## From Scratch
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression, SGDClassifier
from sklearn.datasets import make_classification, make_blobs, make_circles

X, y = make_classification(n_samples=1000,
                           n_features=2,
                           n_redundant=0,
                           n_classes=2,
                           n_clusters_per_class=1,
                           class_sep=2,
                           random_state=12)

# Assign colors to classes (0 for blue, 1 for red)
colors = np.array(['blue', 'red'])

plt.scatter(X[:, 0], X[:, 1], c=colors[y])
plt.show()

model = LogisticRegression()
model.fit(X, y)

x1_min, x2_min = X.min(0)
x1_max, x2_max = X.max(0)

# print(x1_min, x2_min, x1_max, x2_max)

n = 500
x1r = np.linspace(x1_min, x1_max, n)
x2r = np.linspace(x2_min, x2_max, n)
x1m, x2m = np.meshgrid(x1r, x2r)

# print(x1m.shape, x2m.shape)

Xm = np.stack((x1m.flatten(), x2m.flatten()), axis=1)

ym = model.decision_function(Xm)

# ym.shape

# Assign colors to classes (0 for blue, 1 for red)
colors = np.array(['blue', 'red'])

plt.scatter(X[:, 0], X[:, 1], c=colors[y])
plt.contour(x1m, x2m, ym.reshape(x1m.shape), levels=[-1,0,1])
plt.show()

"""## mlxtend"""

from mlxtend.plotting import plot_decision_regions

plot_decision_regions(X, y, clf=model)